{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Jdc5F2dJuH4_ORMBjvoj0QwcLZjfksI1",
      "authorship_tag": "ABX9TyODnBack61mrY1FqIWGDYm0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hxiufan/Capstone713/blob/main/Multi_page_Webscrapping_CorrectionsNZ_23Dec2024_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "X9yEOz4FYfWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCacYV5XsrVL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8GzU4nZStF8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "836dfde7-b2e4-4b06-a638-da101c8e7402",
        "outputId": "78c8f814-949b-4aa0-deea-66569c1863bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 926 snapshots.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping snapshots:  76%|███████▌  | 706/926 [31:42<08:21,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network error while scraping https://web.archive.org/web/20221006040815/https://corrections.govt.nz/: HTTPSConnectionPool(host='web.archive.org', port=443): Read timed out. (read timeout=30)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping snapshots:  88%|████████▊ | 819/926 [39:03<05:41,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network error while scraping https://web.archive.org/web/20240117071144/https://corrections.govt.nz/: HTTPSConnectionPool(host='web.archive.org', port=443): Read timed out. (read timeout=30)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rScraping snapshots:  89%|████████▊ | 820/926 [39:34<20:23, 11.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network error while scraping https://web.archive.org/web/20240117150616/http://www.corrections.govt.nz/: HTTPSConnectionPool(host='web.archive.org', port=443): Read timed out. (read timeout=30)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping snapshots: 100%|██████████| 926/926 [44:54<00:00,  2.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to /content/drive/MyDrive/DA713/wayback_snapshots_with_timestamps_23Dec2024_corrections_v3.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Updated Code to Keep Wayback Machine Timestamp and Format It\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to query Wayback Machine API and get all snapshots within a date range\n",
        "def get_wayback_snapshots(url, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Query the Wayback Machine API for all snapshots of a URL within a date range.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL to query.\n",
        "        start_date (str): Start date in YYYYMMDD format.\n",
        "        end_date (str): End date in YYYYMMDD format.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing snapshot URLs and timestamps.\n",
        "    \"\"\"\n",
        "    wayback_api_url = f\"http://web.archive.org/cdx/search/cdx?url={url}&output=json&collapse=digest&from={start_date}&to={end_date}\"\n",
        "    try:\n",
        "        response = requests.get(wayback_api_url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        snapshots = response.json()[1:]  # Skip the header row\n",
        "        return [\n",
        "            {\"snapshot_url\": f\"https://web.archive.org/web/{snapshot[1]}/{snapshot[2]}\", \"timestamp\": snapshot[1]}\n",
        "            for snapshot in snapshots\n",
        "        ]\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching Wayback Machine snapshots: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to get SEO features and additional metadata from a webpage\n",
        "def get_seo_features(url, session):\n",
        "    \"\"\"\n",
        "    Extract SEO features and additional metadata from a webpage.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the webpage.\n",
        "        session (requests.Session): The session for making requests.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary of SEO data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = session.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "        # Extract metadata\n",
        "        keywords = soup.find('meta', attrs={'name': 'dc.keywords'})\n",
        "        description = soup.find('meta', attrs={'name': 'dc.description'})\n",
        "        dc_description = soup.find('meta', attrs={'name': 'dc.description'})\n",
        "\n",
        "        return {\n",
        "            'title': soup.find('title').text.strip() if soup.find('title') else 'No title found',\n",
        "            'meta_description': description['content'].strip() if description else 'No description found',\n",
        "            'h1_tags': [h1.text.strip() for h1 in soup.find_all('h1')],\n",
        "            'word_count': len(soup.get_text(strip=True).split()),\n",
        "            'keywords': keywords['content'].strip() if keywords else 'No keywords found',\n",
        "            'description': description['content'].strip() if description else 'No description found',\n",
        "            'DC.Description': dc_description['content'].strip() if dc_description else 'No DC.Description found'\n",
        "        }\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Network error while scraping {url}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Parsing error for {url}: {e}\")\n",
        "    return {\n",
        "        'title': 'Error',\n",
        "        'meta_description': 'Error',\n",
        "        'h1_tags': 'Error',\n",
        "        'word_count': 0,\n",
        "        'keywords': 'Error',\n",
        "        'description': 'Error',\n",
        "        'DC.Description': 'Error'\n",
        "    }\n",
        "\n",
        "# Function to save data to CSV\n",
        "def save_to_csv(data, filename):\n",
        "    \"\"\"\n",
        "    Save data to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        data (list): List of dictionaries containing data.\n",
        "        filename (str): Output CSV filename.\n",
        "    \"\"\"\n",
        "    pd.DataFrame(data).to_csv(filename, index=False, encoding='utf-8')\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "# Main function to scrape all snapshots from Wayback Machine\n",
        "def scrape_wayback_pages(original_url, start_date, end_date, output_filename=\"/content/drive/MyDrive/DA713/wayback_filtered_snapshots_corrections_data.csv\"):\n",
        "    \"\"\"\n",
        "    Scrape all snapshots from Wayback Machine for a given URL and save SEO data to a CSV.\n",
        "\n",
        "    Parameters:\n",
        "        original_url (str): The URL to scrape snapshots for.\n",
        "        start_date (str): Start date in YYYYMMDD format.\n",
        "        end_date (str): End date in YYYYMMDD format.\n",
        "        output_filename (str): The name of the output CSV file.\n",
        "    \"\"\"\n",
        "    snapshots = get_wayback_snapshots(original_url, start_date, end_date)\n",
        "    print(f\"Found {len(snapshots)} snapshots.\")\n",
        "\n",
        "    all_data = []\n",
        "    session = requests.Session()  # Reuse session for efficiency\n",
        "\n",
        "    for snapshot in tqdm(snapshots, desc=\"Scraping snapshots\"):\n",
        "        snapshot_url = snapshot['snapshot_url']\n",
        "        original_timestamp = snapshot['timestamp']\n",
        "\n",
        "        # Convert Wayback Machine timestamp to human-readable format\n",
        "        formatted_timestamp = datetime.strptime(original_timestamp, \"%Y%m%d%H%M%S\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        seo_data = get_seo_features(snapshot_url, session)\n",
        "        seo_data['snapshot_url'] = snapshot_url\n",
        "        seo_data['Timestamp'] = formatted_timestamp  # Use formatted timestamp\n",
        "        all_data.append(seo_data)\n",
        "        time.sleep(1)  # Be polite to the server\n",
        "\n",
        "    save_to_csv(all_data, output_filename)\n",
        "\n",
        "# Use \"https://www.corrections.govt.nz/\" as the targe page\n",
        "original_url = \"https://www.corrections.govt.nz/\"\n",
        "start_date = \"20150101\"  # Start date: 01 Jan 2015\n",
        "end_date = \"20241130\"    # End date: 30 Nov 2024\n",
        "scrape_wayback_pages(original_url, start_date, end_date, output_filename=\"/content/drive/MyDrive/DA713/wayback_snapshots_with_timestamps_23Dec2024_corrections_v3.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UzRWe4d0lxvl"
      }
    }
  ]
}