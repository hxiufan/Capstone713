{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b64fd-e407-49ce-b917-fa71510fc5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Code to Keep Wayback Machine Timestamp and Format It\n",
    "# StudyLink\n",
    "# https://www.studylink.govt.nz/\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to query Wayback Machine API and get all snapshots within a date range\n",
    "def get_wayback_snapshots(url, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Query the Wayback Machine API for all snapshots of a URL within a date range.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL to query.\n",
    "        start_date (str): Start date in YYYYMMDD format.\n",
    "        end_date (str): End date in YYYYMMDD format.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries containing snapshot URLs and timestamps.\n",
    "    \"\"\"\n",
    "    wayback_api_url = f\"http://web.archive.org/cdx/search/cdx?url={url}&output=json&collapse=digest&from={start_date}&to={end_date}\"\n",
    "    try:\n",
    "        response = requests.get(wayback_api_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        snapshots = response.json()[1:]  # Skip the header row\n",
    "        return [\n",
    "            {\"snapshot_url\": f\"https://web.archive.org/web/{snapshot[1]}/{snapshot[2]}\", \"timestamp\": snapshot[1]}\n",
    "            for snapshot in snapshots\n",
    "        ]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching Wayback Machine snapshots: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to get SEO features and additional metadata from a webpage\n",
    "def get_seo_features(url, session):\n",
    "    \"\"\"\n",
    "    Extract SEO features and additional metadata from a webpage.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL of the webpage.\n",
    "        session (requests.Session): The session for making requests.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary of SEO data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        # Extract metadata\n",
    "        keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
    "        description = soup.find('meta', attrs={'name': 'description'})\n",
    "        dc_description = soup.find('meta', attrs={'name': 'DC.Description'})\n",
    "        \n",
    "        return {\n",
    "            'title': soup.find('title').text.strip() if soup.find('title') else 'No title found',\n",
    "            'h1_tags': [h1.text.strip() for h1 in soup.find_all('h1')],\n",
    "            'word_count': len(soup.get_text(strip=True).split()),\n",
    "            'keywords': keywords['content'].strip() if keywords else 'No keywords found',\n",
    "            'description': description['content'].strip() if description else 'No description found',\n",
    "            'DC.Description': dc_description['content'].strip() if dc_description else 'No description found'\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error while scraping {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error for {url}: {e}\")\n",
    "    return {\n",
    "        'title': 'Error',\n",
    "        'h1_tags': 'Error',\n",
    "        'word_count': 0,\n",
    "        'keywords': 'Error',\n",
    "        'description': 'Error',\n",
    "        'DC.Description': 'Error'\n",
    "    }\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data, filename):\n",
    "    \"\"\"\n",
    "    Save data to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        data (list): List of dictionaries containing data.\n",
    "        filename (str): Output CSV filename.\n",
    "    \"\"\"\n",
    "    pd.DataFrame(data).to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Main function to scrape all snapshots from Wayback Machine\n",
    "def scrape_wayback_pages(original_url, start_date, end_date, output_filename=\"wayback_filtered_snapshots_data.csv\"):\n",
    "    \"\"\"\n",
    "    Scrape all snapshots from Wayback Machine for a given URL and save SEO data to a CSV.\n",
    "    \n",
    "    Parameters:\n",
    "        original_url (str): The URL to scrape snapshots for.\n",
    "        start_date (str): Start date in YYYYMMDD format.\n",
    "        end_date (str): End date in YYYYMMDD format.\n",
    "        output_filename (str): The name of the output CSV file.\n",
    "    \"\"\"\n",
    "    snapshots = get_wayback_snapshots(original_url, start_date, end_date)\n",
    "    print(f\"Found {len(snapshots)} snapshots.\")\n",
    "    \n",
    "    all_data = []\n",
    "    session = requests.Session()  # Reuse session for efficiency\n",
    "    \n",
    "    for snapshot in tqdm(snapshots, desc=\"Scraping snapshots\"):\n",
    "        snapshot_url = snapshot['snapshot_url']\n",
    "        original_timestamp = snapshot['timestamp']\n",
    "        \n",
    "        # Convert Wayback Machine timestamp to human-readable format\n",
    "        formatted_timestamp = datetime.strptime(original_timestamp, \"%Y%m%d%H%M%S\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        seo_data = get_seo_features(snapshot_url, session)\n",
    "        seo_data['snapshot_url'] = snapshot_url\n",
    "        seo_data['Timestamp'] = formatted_timestamp  # Use formatted timestamp\n",
    "        all_data.append(seo_data)\n",
    "        time.sleep(1)  # Be polite to the server\n",
    "    \n",
    "    save_to_csv(all_data, output_filename)\n",
    "\n",
    "# Example usage\n",
    "original_url = \"https://www.studylink.govt.nz/\"\n",
    "start_date = \"20150101\"  # Start date: 01 Jan 2015\n",
    "end_date = \"20241130\"    # End date: 30 Nov 2024\n",
    "scrape_wayback_pages(original_url, start_date, end_date, output_filename=\"wayback_snapshots_with_timestamps_23Dec2024_StudyLink.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccbafc4-9cef-4661-b872-292ce0415cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Code to Keep Wayback Machine Timestamp and Format It\n",
    "# Work and Income\n",
    "# https://www.workandincome.govt.nz/\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to query Wayback Machine API and get all snapshots within a date range\n",
    "def get_wayback_snapshots(url, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Query the Wayback Machine API for all snapshots of a URL within a date range.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL to query.\n",
    "        start_date (str): Start date in YYYYMMDD format.\n",
    "        end_date (str): End date in YYYYMMDD format.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries containing snapshot URLs and timestamps.\n",
    "    \"\"\"\n",
    "    wayback_api_url = f\"http://web.archive.org/cdx/search/cdx?url={url}&output=json&collapse=digest&from={start_date}&to={end_date}\"\n",
    "    try:\n",
    "        response = requests.get(wayback_api_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        snapshots = response.json()[1:]  # Skip the header row\n",
    "        return [\n",
    "            {\"snapshot_url\": f\"https://web.archive.org/web/{snapshot[1]}/{snapshot[2]}\", \"timestamp\": snapshot[1]}\n",
    "            for snapshot in snapshots\n",
    "        ]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching Wayback Machine snapshots: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to get SEO features and additional metadata from a webpage\n",
    "def get_seo_features(url, session):\n",
    "    \"\"\"\n",
    "    Extract SEO features and additional metadata from a webpage.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL of the webpage.\n",
    "        session (requests.Session): The session for making requests.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary of SEO data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        # Extract metadata\n",
    "        keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
    "        description = soup.find('meta', attrs={'name': 'description'})\n",
    "        dc_description = soup.find('meta', attrs={'name': 'DC.Description'})\n",
    "        \n",
    "        return {\n",
    "            'title': soup.find('title').text.strip() if soup.find('title') else 'No title found',\n",
    "            'h1_tags': [h1.text.strip() for h1 in soup.find_all('h1')],\n",
    "            'word_count': len(soup.get_text(strip=True).split()),\n",
    "            'keywords': keywords['content'].strip() if keywords else 'No keywords found',\n",
    "            'description': description['content'].strip() if description else 'No description found',\n",
    "            'DC.Description': dc_description['content'].strip() if dc_description else 'No description found'\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error while scraping {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error for {url}: {e}\")\n",
    "    return {\n",
    "        'title': 'Error',\n",
    "        'h1_tags': 'Error',\n",
    "        'word_count': 0,\n",
    "        'keywords': 'Error',\n",
    "        'description': 'Error',\n",
    "        'DC.Description': 'Error'\n",
    "    }\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data, filename):\n",
    "    \"\"\"\n",
    "    Save data to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        data (list): List of dictionaries containing data.\n",
    "        filename (str): Output CSV filename.\n",
    "    \"\"\"\n",
    "    pd.DataFrame(data).to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Main function to scrape all snapshots from Wayback Machine\n",
    "def scrape_wayback_pages(original_url, start_date, end_date, output_filename=\"wayback_filtered_snapshots_data.csv\"):\n",
    "    \"\"\"\n",
    "    Scrape all snapshots from Wayback Machine for a given URL and save SEO data to a CSV.\n",
    "    \n",
    "    Parameters:\n",
    "        original_url (str): The URL to scrape snapshots for.\n",
    "        start_date (str): Start date in YYYYMMDD format.\n",
    "        end_date (str): End date in YYYYMMDD format.\n",
    "        output_filename (str): The name of the output CSV file.\n",
    "    \"\"\"\n",
    "    snapshots = get_wayback_snapshots(original_url, start_date, end_date)\n",
    "    print(f\"Found {len(snapshots)} snapshots.\")\n",
    "    \n",
    "    all_data = []\n",
    "    session = requests.Session()  # Reuse session for efficiency\n",
    "    \n",
    "    for snapshot in tqdm(snapshots, desc=\"Scraping snapshots\"):\n",
    "        snapshot_url = snapshot['snapshot_url']\n",
    "        original_timestamp = snapshot['timestamp']\n",
    "        \n",
    "        # Convert Wayback Machine timestamp to human-readable format\n",
    "        formatted_timestamp = datetime.strptime(original_timestamp, \"%Y%m%d%H%M%S\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        seo_data = get_seo_features(snapshot_url, session)\n",
    "        seo_data['snapshot_url'] = snapshot_url\n",
    "        seo_data['Timestamp'] = formatted_timestamp  # Use formatted timestamp\n",
    "        all_data.append(seo_data)\n",
    "        time.sleep(1)  # Be polite to the server\n",
    "    \n",
    "    save_to_csv(all_data, output_filename)\n",
    "\n",
    "# Example usage\n",
    "original_url = \"https://www.workandincome.govt.nz/\"\n",
    "start_date = \"20150101\"  # Start date: 01 Jan 2015\n",
    "end_date = \"20241130\"    # End date: 30 Nov 2024\n",
    "scrape_wayback_pages(original_url, start_date, end_date, output_filename=\"wayback_snapshots_with_timestamps_23Dec2024_WorkAndIncome.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ee4b1-888a-43a8-97ec-676d075b4fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
