{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hxiufan/Capstone713/blob/main/Multi_page_Web_Scrapping_05Dec2024_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "vLaoHXKVnfX-"
      },
      "id": "vLaoHXKVnfX-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n"
      ],
      "metadata": {
        "id": "T-um5Sdzn-zJ"
      },
      "id": "T-um5Sdzn-zJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "B5Eun0mdoBsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dacc2c85-b42d-435b-9ac5-4f67dd5b1113"
      },
      "id": "B5Eun0mdoBsX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee97e069-3d3c-4dde-90cd-22086198f31a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee97e069-3d3c-4dde-90cd-22086198f31a",
        "outputId": "cd666aa3-9587-4026-d3f2-2099f0330212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 634 snapshots.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping snapshots:   4%|▍         | 28/634 [01:11<26:23,  2.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network error while scraping https://web.archive.org/web/20050816210600/http://www.studylink.govt.nz:80/: 400 Client Error: Bad request for url: https://web.archive.org/web/20050816210600/http://www.studylink.govt.nz:80/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping snapshots:  53%|█████▎    | 338/634 [14:46<11:12,  2.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network error while scraping https://web.archive.org/web/20160707213313/http://www.studylink.govt.nz:80/: 400 Client Error: Bad Request for url: https://web.archive.org/web/20160707213313/http://www.studylink.govt.nz:80/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping snapshots:  56%|█████▌    | 353/634 [15:23<11:33,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network error while scraping https://web.archive.org/web/20161002040608/http://studylink.govt.nz/: HTTPSConnectionPool(host='web.archive.org', port=443): Read timed out. (read timeout=30)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping snapshots: 100%|██████████| 634/634 [28:18<00:00,  2.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to /content/drive/MyDrive/DA713/wayback_snapshots_2019_2023_with_meta.csv\n"
          ]
        }
      ],
      "source": [
        "# Function to query Wayback Machine API and get all snapshots within a date range\n",
        "def get_wayback_snapshots(url, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Query the Wayback Machine API for all snapshots of a URL within a date range.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL to query.\n",
        "        start_date (str): Start date in YYYYMMDD format.\n",
        "        end_date (str): End date in YYYYMMDD format.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of Wayback Machine URLs for all snapshots.\n",
        "    \"\"\"\n",
        "    wayback_api_url = f\"http://web.archive.org/cdx/search/cdx?url={url}&output=json&collapse=digest&from={start_date}&to={end_date}\"\n",
        "    try:\n",
        "        response = requests.get(wayback_api_url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        snapshots = response.json()[1:]  # Skip the header row\n",
        "        return [\n",
        "            f\"https://web.archive.org/web/{snapshot[1]}/{snapshot[2]}\"\n",
        "            for snapshot in snapshots\n",
        "        ]\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching Wayback Machine snapshots: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to get SEO features and additional meta-data from a webpage\n",
        "def get_seo_features(url, session):\n",
        "    \"\"\"\n",
        "    Extract SEO features and additional metadata from a webpage.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the webpage.\n",
        "        session (requests.Session): The session for making requests.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary of SEO data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = session.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "        # Extract the additional metadata\n",
        "        keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
        "        description = soup.find('meta', attrs={'name': 'description'})\n",
        "        dc_description = soup.find('meta', attrs={'name': 'DC.Description'})\n",
        "        dc_language = soup.find('meta', attrs={'name': 'DC.Language'})\n",
        "        dc_date_created = soup.find('meta', attrs={'name': 'DC.Date.created'})\n",
        "        dc_date_modified = soup.find('meta', attrs={'name': 'DC.Date.modified'})\n",
        "        dc_date_valid = soup.find('meta', attrs={'name': 'DC.Date.valid'})\n",
        "\n",
        "        return {\n",
        "            'page_url': url,\n",
        "            'title': soup.find('title').text.strip() if soup.find('title') else 'No title found',\n",
        "            'meta_description': description['content'].strip() if description else 'No description found',\n",
        "            'h1_tags': [h1.text.strip() for h1 in soup.find_all('h1')],\n",
        "            'word_count': len(soup.get_text(strip=True).split()),\n",
        "            'keywords': keywords['content'].strip() if keywords else 'No keywords found',\n",
        "            'description': description['content'].strip() if description else 'No description found',\n",
        "            'DC.Description': dc_description['content'].strip() if dc_description else 'No DC.Description found',\n",
        "            'DC.Language': dc_language['content'].strip() if dc_language else 'No DC.Language found',\n",
        "            'DC.Date.created': dc_date_created['content'].strip() if dc_date_created else 'No DC.Date.created found',\n",
        "            'DC.Date.modified': dc_date_modified['content'].strip() if dc_date_modified else 'No DC.Date.modified found',\n",
        "            'DC.Date.valid': dc_date_valid['content'].strip() if dc_date_valid else 'No DC.Date.valid found'\n",
        "        }\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Network error while scraping {url}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Parsing error for {url}: {e}\")\n",
        "    return {\n",
        "        'page_url': url,\n",
        "        'title': 'Error',\n",
        "        'meta_description': 'Error',\n",
        "        'h1_tags': 'Error',\n",
        "        'word_count': 0,\n",
        "        'keywords': 'Error',\n",
        "        'description': 'Error',\n",
        "        'DC.Description': 'Error',\n",
        "        'DC.Language': 'Error',\n",
        "        'DC.Date.created': 'Error',\n",
        "        'DC.Date.modified': 'Error',\n",
        "        'DC.Date.valid': 'Error'\n",
        "    }\n",
        "\n",
        "# Function to save data to CSV\n",
        "def save_to_csv(data, filename):\n",
        "    \"\"\"\n",
        "    Save data to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        data (list): List of dictionaries containing data.\n",
        "        filename (str): Output CSV filename.\n",
        "    \"\"\"\n",
        "    pd.DataFrame(data).to_csv(filename, index=False, encoding='utf-8')\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "# Main function to scrape all snapshots from Wayback Machine\n",
        "def scrape_wayback_pages(original_url, start_date, end_date, output_filename=\"/content/drive/MyDrive/DA713/wayback_filtered_snapshots_data.csv\"):\n",
        "    \"\"\"\n",
        "    Scrape all snapshots from Wayback Machine for a given URL and save SEO data to a CSV.\n",
        "\n",
        "    Parameters:\n",
        "        original_url (str): The URL to scrape snapshots for.\n",
        "        start_date (str): Start date in YYYYMMDD format.\n",
        "        end_date (str): End date in YYYYMMDD format.\n",
        "        output_filename (str): The name of the output CSV file.\n",
        "    \"\"\"\n",
        "    snapshot_urls = get_wayback_snapshots(original_url, start_date, end_date)\n",
        "    print(f\"Found {len(snapshot_urls)} snapshots.\")\n",
        "\n",
        "    all_data = []\n",
        "    session = requests.Session()  # Reuse session for efficiency\n",
        "\n",
        "    for snapshot_url in tqdm(snapshot_urls, desc=\"Scraping snapshots\"):\n",
        "        seo_data = get_seo_features(snapshot_url, session)\n",
        "        all_data.append(seo_data)\n",
        "        time.sleep(1)  # Be polite to the server\n",
        "\n",
        "    save_to_csv(all_data, output_filename)\n",
        "\n",
        "# Example usage\n",
        "original_url = \"https://www.studylink.govt.nz/\"\n",
        "start_date = \"20020524\"  # Start date: 01 Jan 2002\n",
        "end_date = \"20241130\"    # End date: 30 Nov 2023\n",
        "scrape_wayback_pages(original_url, start_date, end_date, output_filename=\"/content/drive/MyDrive/DA713/wayback_snapshots_2019_2023_with_meta.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}